{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0e134e05457d34029b6460cd73bbf1ed73f339b5b6d98c95be70b69eba114fe95",
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA ( Latent Semantic Analysis  = Latent Semantic Indexing (LSI))\n",
    "- tf-idf : can't consider the meaning of vocab\n",
    "\n",
    "- SVD (singular value decomposition) : A  - U * Σ *(transopose of V) \n",
    "\n",
    "1. U : m * n orthogonal matrix \n",
    "2. V : n * n orthogonal matrix\n",
    "3. diagonal matrix Σ : m * n diagonal matrix\n",
    "3.1 singular value : value of Σ diagonal element  by SVD \n",
    "- orthogonal matrix = inverse matrix \n",
    "- Truncated SVD means decrese the dimension of data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "A=np.array([[0,0,0,1,0,1,1,0,0],[0,0,0,1,1,0,1,0,0],[0,1,1,0,2,0,0,0,0],[1,0,0,0,0,0,0,1,1]])\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.24  0.75  0.   -0.62]\n [-0.51  0.44 -0.    0.74]\n [-0.83 -0.49 -0.   -0.27]\n [-0.   -0.    1.    0.  ]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "U, s, VT = np.linalg.svd(A, full_matrices = True) #return list of singular value \n",
    "print(U.round(2))\n",
    "np.shape(U)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2.69 2.05 1.73 0.77]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "print(s.round(2))\n",
    "np.shape(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.  , -0.31, -0.31, -0.28, -0.8 , -0.09, -0.28, -0.  , -0.  ],\n",
       "       [ 0.  , -0.24, -0.24,  0.58, -0.26,  0.37,  0.58, -0.  , -0.  ],\n",
       "       [ 0.58, -0.  ,  0.  ,  0.  , -0.  ,  0.  , -0.  ,  0.58,  0.58],\n",
       "       [ 0.  , -0.35, -0.35,  0.16,  0.25, -0.8 ,  0.16, -0.  , -0.  ],\n",
       "       [-0.  , -0.78, -0.01, -0.2 ,  0.4 ,  0.4 , -0.2 ,  0.  ,  0.  ],\n",
       "       [-0.29,  0.31, -0.78, -0.24,  0.23,  0.23,  0.01,  0.14,  0.14],\n",
       "       [-0.29, -0.1 ,  0.26, -0.59, -0.08, -0.08,  0.66,  0.14,  0.14],\n",
       "       [-0.5 , -0.06,  0.15,  0.24, -0.05, -0.05, -0.19,  0.75, -0.25],\n",
       "       [-0.5 , -0.06,  0.15,  0.24, -0.05, -0.05, -0.19, -0.25,  0.75]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "VT.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "S = np.zeros((4, 9)) \n",
    "S[:4, :4] = np.diag(s)  # insert singular value to diagonal matrix\n",
    "print(S.round(2))\n",
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "print(VT.round(2))\n",
    "np.shape(VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "np.allclose(A, np.dot(np.dot(U,S), VT).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[2.69 0.  ]\n [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "# Truncated SVD\n",
    "S=S[:2,:2]\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.24  0.75]\n [-0.51  0.44]\n [-0.83 -0.49]\n [-0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "U=U[:,:2]\n",
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "VT=VT[:2,:]\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n [0 0 0 1 1 0 1 0 0]\n [0 1 1 0 2 0 0 0 0]\n [1 0 0 0 0 0 0 1 1]]\n[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "A_prime=np.dot(np.dot(U,S), VT)\n",
    "print(A)\n",
    "print(A_prime.round(2))\n",
    "#VT : 2*9 (number of topic * vocab size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "l\\none dollar for every star in the Milky Way Galaxy.',\n",
       " '\\n\\nOn my LC (RZ to any ex-colonists) I replaced the bolt at the bottom of the barrel\\nwith a tap. When I wanted a coffee I could just rev the engine until boiling\\nand pour out a cup of hot water.\\nI used ethylene glycol as antifreeze rather than methanol as it tastes sweeter.\\n\\n(-:',\n",
       " \"There is another useful method based on Least Sqyares Estimation of the sphere equation parameters.\\n\\nThe points (x,y,z) on a spherical surface with radius R and center (a,b,c) can be written as \\n\\n   (x-a)^2 + (y-b)^2 + (z-c)^2 = R^2\\n\\nThis equation can be rewritten into the following form:  \\n\\n   2ax + 2by + 2cz + R^2 - a^2 - b^2 -c^2 = x^2 + y^2 + z^2\\n\\nApproximate the left hand part by   F(x,y,z) = p1.x + p2.x + p3.z + p4.1\\n\\nFor all datapoints, i.c. 4, determine the 4 parameters p1..p4 which minimise the average error |F(x,y,z) - x^2 - y^2 - z^2|^2.\\n\\nIn 'Numerical Recipes in C' can be found algorithms to solve these parameters.\\n\\nThe best fitting sphere will have \\n- center (a,b,c) = (p1/2, p2/2, p3/2)\\n- radius R = sqrt(p4 + a.a + b.b + c.c).\\n\\nSo, at last, will this solve you sphere estination problem, at least for the most situations I think ?.\",\n",
       " \"Boy, hats off to any Cubs fan who can actually muster up the courage to put\\ndown Braves fans.  I mean, all the Braves have done is gone to two consecutive\\nworld series.  Also, being the Cubs fan that I am, I really have to hand it to\\nall the Braves fans out there that are capable of driving me crazy with that\\ninfernal cheer that they have.  \\n\\nHowever, I do have to protest anyone saying that all Cubs fans are stupid.  The\\nway I see it, either I'm just too stupid to acknowledge it, or that observation\\nwas just plain wrong.  You might have us confused with Bear fans. ;)\\n\\nAnyway, about a two weeks ago just about everyone was saying that the Cubs\\nwould finish up last in their division.  (Even behind Florida?!?  Sheesh!)  \\nThese same people were predicting the Braves to clean up in their respective\\ndivision.  Well,  we're ten games into the season and these people are a little\\nless vocal now.  I wonder why.\\n\\nWell, the way I see it, the East is up for grabs, and whoever wants it most is\\ngoing to take it, with the exception of Florida.  Every team seems to have\\ngood batting and pitching, with Philly presently leading the pack.  But, I just\\nhave to point out, if the Cubs do take the East, they'll do it without the\\nbenefit of a competent manager.  However, and it pains me to say it, the\\npennant is going to go to the West.\\n\\nJust had to get that off my chest.\\n\\n\\t\\t\\t\\t\\t\\tDoc\",\n",
       " \"\\n\\n\\nI second that suggestion.  Although I don't own the HP Portable Deskjet,\\nI *do* own the HP Deskjet 500.  It gives the nicest outputs, with only\\na minor loss of quality.   For all intensive purposes (papers, letters,\\nresumes), I treat my Deskjet like a laser printer (You *do* have to\\nlook a bit closely to see the blurs in the characters).  Only one\\ngrudge, the ink that HP gives you does smudge rather quickly in the\\npresence of moisture (Even though the ink is waterproof).  However,\\nyou would have to spend about $500 more for laser quality.\\n\\nThe cannon bubblejets are nice, however they don't seem to have as\\nmuch support (eg: drivers for popular programs) as the HP ink-jets.\\nAlso HP Deskjet (regular,plus,500,500c) accepts xerox paper (I believe\\nthat the cut-sheet feeder is an option for the cannon bubblejet).\\n\\nIf you don't mind refilling your printer with cheap ink (say\\nfountain-pen ink), then the HP deskjets are *very* cheap to maintain\\n(I paid $3.50 for my last bottle of ink and I expect it to last 9\\nmonths). \\n\\n\\t-Kimball (who doesn't work for HP, but just loves his printer\\n\\t\\t  very much!)\",\n",
       " '',\n",
       " \"\\n\\nHow many NuBus slots do you have?\\n\\nApplied Engineering has something called the QuadraLink, which is\\na card with 4 serial ports that you get at through the comms\\ntoolbox (in addition to the built-in ones) It also comes with\\nsoftware for fooling applications to open an AE port when they\\nthink they open a built-in port.\\n\\nThey also have a more expensive card with DMA (better performance)\\nand I _think_ they, or someone else, have a card that handles\\n8 ports simultaneously.\\n\\nAs I said, with NuBus, you're green. Learn how to use the Comms\\nResource Manager to get at the various installed cards.\\n\\nCheers,\\n\\n\\t\\t\\t\\t\\t/ h+\\n \",\n",
       " '\\n\\nNo, there\\'s no evidence that would convince any but the most credulous.\\n\\nThe \"evidence\" is identical to the sort of evidence that has been\\nused to justify all sorts of quack treatments for quack diseases\\nin the past.\\n\\n\\nI like the implication here.  It must not be that the quacks making\\nmillions off such \"diseases\" are biased -- rather that those who\\ndoubt their existence don\\'t understand the real world.  It seems\\neasy to picture a 19th centure snake oil salesman saying the same\\nthing.\\n\\nHowever, I have been in the trenches long enough to have seen multiple\\nquack diseases rise and fall in popularity.  \"Systemic yeast syndome\"\\nseems to be making a resurgence (it had fallen off a few years ago).\\nThere will be new such \"diseases\" I\\'m sure with best-selling books\\nand expensive therapies.\\n\\n\\nWell this, of course, is convincing.  I guess I\\'d better start diagnosing\\nany illnesses that people want so that I can keep my lips.',\n",
       " 'Is there a FAQ on Cyrix 486DLC? Could anyone please repost it or\\nemail to me, if I missed it? Thanks in advance.\\n \\n\\n... Alexander Poylisher, Internet: apoylis@inode.com; FidoNet: 1:2603/106',\n",
       " '\\n\\n\\tThe word that is missing in this whole discourse is not the \"B\"\\nword, or the \"H\" word, or even the \"N\" or \"W\" words. It is the \"L\" word -\\nLOSER !!\\n\\n\\tThat\\'s right. When we boil all the crap out of this argument, it\\nis all about WINNING and LOSING, and nothing else. Let me explain.\\n\\n\\tRemember the eighties ? No excuses. Nobody who can handle a mail\\nbuffer can claim they are \"too young\" to remember Ronald Reagan - yet.\\nThe eighties were about \"How America Learned to Win Once Again\". Then\\n(wouldn\\'t you know), we won so well that there was nothing left to win.\\nNo Cold War to endure. No nuclear holocaust. No more worlds to conquer\\n(We forgot about outer space long ago). The kind of overwhelming, no\\nholds barred success that killed Alexander the Great. Yes, there were a\\nfew \"little\" problems along the way - stock market meltdown here, an\\nS&L bailout there, a few revolts and crazy Middle Eastern dictators to\\ncontend with, but as Tacitus would tell ya\\', the God Augustus never had\\nit so good. \\n\\n\\tIn the meantime, there is guilt for winning, maybe a fear that one\\ndoesn\\'t deserve one\\'s bounty - or success. So there is a \"kinder and gentler\\ntype of politician these days, Bill Clinton, affirmative action, and lots of\\ndiscourse about people who \"don\\'t get it\". For those of us in the winning\\nbusiness, this kind of talk is mildly irritating, but there is still no \\nsuggestion of losing.\\n\\n\\tBut what do we find now ? To put it mildy, the stereotype of our \\n\"white male\" non-winner is Woody Hayes in the Rose Bowl, punching out \\nphotojournalists when those California fruits and nuts steal another one\\nwith a \"Hail Mary\" pass in the Fourth Quarter. (The whole idea behind \\'three\\nyards and a cloud of dust\\' is to wear your opponent down until he collapses\\nin the final period) But Woody just used his fists - Uzzies seem to be the \\nweapon of choice these days. \\n\\t\\n\\tWho is D-FENS, anyway ? The answer is as plain as the horn rims on \\nyour face. The guy is MICHAEL DOUGLAS, posing as a LOSER. This \\nis known as controversial casting. But that baggy short-sleeved white shirt \\nsure does look natural on Mike doesn\\'t it. Gordon Gekko will never look the \\nsame. (Though Woody always dressed that way.) Did we really expect Gekko to \\ntake it easy and enjoy that kind of wardrobe, without putting up a fuss ?\\n\\n\\tWhat we are starting to lose sight of is, that bashing D-FENS is \\nthe same game as bashing that poor African American slug that Clint Eastwood\\nused to blow away all the time. As that arch-WASP (male gender) George C. Scott\\ndeclaimed, \"Americans traditionally LOVE TO WIN. They love a winner, and will \\nnot tolerate a loser.\" And so on. \\n\\n\\tThe political implications are simple. If, as many socialists - and\\nDemocrats - do, you consider society a finite pie to a apportioned in some \\n\"equitable\" way, then you have to worry about who is a winner and who is a \\nloser to tell whose side you are on. That could be black women today, Asian\\nhomosexuals tommorrow, and yes indeed, white men some yet to be determined\\nday when the balance of the pie has finally swung against that (39%) \\nminority.\\n\\n\\tOr you can just blow the whole thing off and say - as do most\\nconservatives and all the libertarians - and act is if you didn\\'t care\\nwho\\'s winning and who\\'s losing. In some cases, you might say something\\nabout make sure the game is fair (equality of opportunity, not of condition).\\nIn the latter case, you might be able to identify yourself as a \\n\"neoconservative\" or a \"neoliberal\" depending on how much you want to limit\\nthe pot.\\n\\n\\tEither way you go, the way of the Winner is no longer the way to be\\npopular - at least after you graduate from High School (but you\\'ll still\\nbe popular at High School reunions). But it beats being a Nerd, as I \\nwould imagine Michael Douglas would now agree, and in the long run, it\\nis the only way to go. (Even in Hollywood, which treats Losers worse than any\\nother place in America except for New York and Washington, D.C. - and even in\\nColumbus, Ohio, which produced Alex Keaton, but no champion football teams in\\nthe eighties and the first quarter of the nineties) I\\'d like to \\nsee more Winners in this society, regardless of race, gender, religious \\npreference, and sexual orientation. Maybe we should even let a few more of \\nthem be white men !! (We should DEFINITELY let the Buckeyes win the Rose Bowl\\nsomeday)\\n\\n\\n\\nBill R.\\n\\n--',\n",
       " \"While playing around with my Gateway 2000 local-bus machine last\\nnight, it became apparent that Windows 3.1 didn't give the option\\nfor 32-bit access for virtual memory.\\n\\nI am using a permanent swap file, and the disk drive is on the local\\nbus interface.\\n\\nIs this expected, or should I be investigating further why no 32-bit\\noption appears?\\n\\nThanks for any help.\\n\\n--\",\n",
       " 'Is there a Wyse 60 Terminal Emulator or a comms toolbox kit available on the\\nnet somewhere?\\n\\nThanks.',\n",
       " \"\\n\\tI suppose ALL media want something to happen, otherwise what would\\n\\tthey report: that's their job. (duhhh to me!)\\n\\n\\tBut it's not so much surprising that they want a riot as it is amazing\\n\\thow they carry that desire across in not so subtle ways (at least to\\n\\tme...)\\n\\ncarlos.\",\n",
       " \"\\nOh... I forgot... Art Shamsky, former Red and Mets player.  Batted .301\\nbetween injuries in 1969 (fell short of qualifying for Top 10 because of\\ninjuries and platoon with Ron Swoboda; no Swobo wasn't Jewish).\",\n",
       " \"I don't mean to be disrespectful to your concerns, but it seems to me \\nthat you're getting all wound up in a non-issue.  \\n\\nAs many knowledgeable people have pointed out, msg is a naturally \\noccurring substance in a lot, if not most, foods.  When food \\nmanufacturers add it to a preparation, they do so because it's a \\nknown flavor enhancer. \\n\\nYour wife's theory, that MSG is added to food to stimulate appetite, \\nmay well be true.  But I don't believe it's ALWAYS the reason it's \\nadded.  People are (largely, for the most part) in charge of their \\nown appetites. \\n\\nYou don't know much about cats, do you? \\n\\nCats will Take Advantage of You.  Resign yourself:  you will never  \\nunderstand a cat.  Their tastes are whimsical.  \\n\\nI also suspect, though it's been a while since I've checked ingredients \\non commercial cat food, that there are much more stringent requirements \\non pet food additives than human.  \\n\\nSee, the FDA has this stupid idea that human beings have the intelligence \\nto look out after their own interests.  \",\n",
       " \"Hello networld,\\n\\nI'm looking for an X mailreader. Is there a Xelm?\\n\\nAndreas\\n\\n\",\n",
       " \"\\n\\n\\nIf by that you mean anything on the GD approach, there was an article on\\nit in a recent Avation Week. I don't remember the exact date but it was\\nrecent.\\n\\n Allen\\n\",\n",
       " \"Any more news on Steve's status since he lost the starting job\\nwould be appreciated\",\n",
       " 'NUT CASE PANICS!!!!JUMPS THE GUN ON THE NET BEFORE GETTING FACTS STRAIGHT!!!!\\n',\n",
       " \"\\n\\nThere is a rite like this described in Joseph Campbell's\\n_Occidental_Mythology_.  He also described levels of initiation, I think\\n6?  I don't know where Campbell got his info, but I remember thinking he\\nwas being a little eclectic.\\n\\n\\nQuite a bit.  If you haven't read Campbell, give him a try.  \\n\",\n",
       " \"[reply to geb@cs.pitt.edu (Gordon Banks)]\\n \\n \\n \\nI made a decision a while back that I will not be bullied into getting\\nstudies like a CT or MRI when I don't think they are indicated.  If the\\npatient won't accept my explanation of why I think the study would be a\\nwaste of time and money, I suggest a second opinion.\",\n",
       " \"I'm looking for some Game Boy games.  Please e-mail me with your list and offers!  Thanks!  Also, if you have a game boy you want to get rid of, please tell me.  \",\n",
       " 'The Chevrolet brothers were respected racers & test drivers for the\\nBuick Co. when Durant was there.\\n\\nWhen the directors kicked Durant out of GM in 1910 he took Chevrolet and\\nothers with him.  As mentioned before, they founded the successful\\nChevrolet company.\\n\\nA little-known fact is that the Chevrolet Co. actually took over GM!\\nThat was how Durant got back in charge of GM-- legally his new company\\nChevrolet Co. did the buying, and GM was a division of Chevrolet!\\n\\nAfter 1920 and into the Sloan era, GM shuffled things so that the GM\\nboard was superior, but there was always a degree of autonomy given\\nthe Chevy division, presumably because of the initial structure.\\n(If you look at the organization chart for GM in Sloan\\'s book, Chevy\\ndivision reports directly to 14th floor, not through the \"passenger\\ncar division\" which covers Buick, Olds, Cadillac, and Oakland/Pontiac)',\n",
       " \"\\n\\tThere is a free program called 'xkernel' which does just that.\\nIt is by Seth Robertson (seth@ctr.columbia.edu).  It takes a sun 3 and\\nboots a limited kernel which allows you to run X.  We converted 4\\nmachines over this semester and the speedup is enormously appreciable\\n-- I find them faster than an NCD 15inch black&white XTerminal that we\\nare playing with, and a bigger screen to boot!  As a matter of fact,\\nthe department just bought some old sun3s at an auction to convert!\\n\\n} Xkernel is available for anonymous ftp from ftp.ctr.columbia.edu\\n} [128.59.64.40] in /Xkernel/Xkernel.shar (/Xkernel is a symlink to the\\n} latest version of Xkernel)\\n\\n\\tNote that the compiled version which is available is for the\\nsun 3/50, but compiling to work for a sun 3/1xx should be quite easy.\\n\\n\\tI am not connected with xkernel except as a satisfied\\ninstaller and user 8).  I may be able to answer questions; feel free\\nto email me.\",\n",
       " '\\nThis is a very good point.  One that I have held for sometime.  We do not\\nallow people to develop on the paths that they choose or desire.  Even with\\nheterosexuals we tend to leave some hanging in the sense of knowledge and\\ninformation about sexuality and relationships.\\n\\nIt is very difficult for a young person to develop and build a positive\\nview of themself when they are constantly being told implicitly and explicitly\\nthat they are wrong and immoral. ',\n",
       " ':    Indeed, if NSA really designed the algorithm to be secure, it\\'s very likely\\n: as secure as IDEA or 2-key DES.  However, the system as a whole isn\\'t resistant\\n: to \"practical cryptanalysis.\"  In _The Puzzle Palace_, Bamford describes how\\n: several NSA employees were turned by foreign (presumably KGB) agents, despite\\n: security measures that I doubt any Big 8 accounting firm could match.  And\\n: NSA confidential data was *not* subject to being requested by thousands of\\n: police organizations and courts across the land.\\n\\nAh yes, don\\'t anyone mention Ronald William Pelton[*], heh heh heh.  How\\nembarrassing.',\n",
       " \"\\n\\nI am told (by the person who I care a lot about and who I am worried\\nis going to start putting his health and money into homeopathy without\\nreally knowing what he is getting into and who is the reason I posted\\nin the first place about homeopathy) that in Britain homeopathy is\\navailable on the National Health Service and that there are about 6000\\nGPs who use homeopathic practices. True? False? What?\\n\\nHave there been any important and documented investigations into\\nhomeopathic principles?\\n\\nI was reading a book on homeopathy over the weekend. I turned to the\\nsection on the principles behind homeopathic medicine, and two\\nparagraphs informed me that homeopaths don't feel obliged to provide\\nany sort of explanation. The author stated this with pride, as though\\nit were some sort of virtue! Why am I sceptical about homeopathy? Is\\nit because I am a narrow-minded bigot, or is it because homeopathy\\nreally looks more like witch-doctory than anything else?\",\n",
       " \"I posted this to the apps group and didn't get any response, so\\nI'll try here. I am trying to use the latex help feature\\navailable in emacs for windows and read that you need a separate\\nlatexhlp.zip file along with a vms2hlp.zip file to convert this\\nto windows help. Has anyone found these files or gotten this\\ncommand help to work?\",\n",
       " '}>More like those who use their backs instead of their minds to make\\n}>their living who are usually ignorant and intolerant of anything outside\\n}>of their group or level of understanding.\\n\\nThere seems to be some confusion between rednecks and white trash.\\nThe confusion is understandable, as there is substantial overlap\\nbetween the two sets. Let me see if I can clarify:\\n\\nRednecks: Primarily use their backs instead of their minds to make a\\n\\tliving. Usually somewhat ignorant (by somebody\\'s standards,\\n\\tanyway) because they have never held education above basic\\n\\treading/writing/math skills to be that important to their\\n\\teventual vocation. Note I did not say stupid, just ignorant.\\n\\t(They might be stupid, but then so are some high percentage\\n\\tof any group).\\n\\nWhite trash: \"White trash fit the stereotype referred to by the\\n\\tword \\'nigger\\' better than any black person I ever met, only\\n\\twith the added \\'bonus\\' that white trash are mean as hell.\"\\n\\t-- my father. Genuinely lazy (not just out of work or under-\\n\\tqualified), good-for-nothing, dishonest, white people who are\\n\\tmean as snakes. The \"squeal like a pig\" boys in _Deliverance_\\n\\tmay or may not have been rednecks, but they were sure as hell\\n\\twhite trash.\\n\\nWhite trash are assuredly intolerant of anything outside of their\\ngroup or level of understanding. Rednecks may or may not be.',\n",
       " '\\nOne should be aware that foreign doctors admitted for training\\nare ineligible to apply for resident alien status.  In order\\nto get the green card they have to return to their country and\\napply at the embassy there.  Of course, many somehow get around\\nthis problem.  Often it is by agreeing to practice in a town\\nwith a need and then the congressman from that district tacks\\na rider onto a bill saying \"Dr. X will be allowed to have permanent\\nresidency in the US.\"  A lot of bills in congress have such riders\\nattached to them.  Marrying a US citizen is the most common, although\\nnow they are even cracking down on that and trying to tell US\\ncitizens they must follow their spouse back to the Phillipines, or\\nwhereever.\\n\\n\\n\\n-- \\n----------------------------------------------------------------------------\\nGordon Banks  N3JXP      | \"Skepticism is the chastity of the intellect, and\\ngeb@cadre.dsl.pitt.edu   |  it is shameful to surrender it too soon.\" ',\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "argument of type 'WordListCorpusReader' is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-46ff726c0b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NLTK로부터 불용어를 받아옵니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenized_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 토큰화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenized_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# 불용어를 제거합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-46ff726c0b47>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NLTK로부터 불용어를 받아옵니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenized_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 토큰화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenized_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# 불용어를 제거합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-46ff726c0b47>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NLTK로부터 불용어를 받아옵니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenized_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 토큰화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenized_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# 불용어를 제거합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'WordListCorpusReader' is not iterable"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english') # NLTK로부터 불용어를 받아옵니다.\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if not item not in stopwords])\n",
    "# 불용어를 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['yeah',\n",
       " 'expect',\n",
       " 'people',\n",
       " 'read',\n",
       " 'actually',\n",
       " 'accept',\n",
       " 'hard',\n",
       " 'atheism',\n",
       " 'need',\n",
       " 'little',\n",
       " 'leap',\n",
       " 'faith',\n",
       " 'jimmy',\n",
       " 'your',\n",
       " 'logic',\n",
       " 'runs',\n",
       " 'steam',\n",
       " 'sorry',\n",
       " 'pity',\n",
       " 'sorry',\n",
       " 'that',\n",
       " 'have',\n",
       " 'these',\n",
       " 'feelings',\n",
       " 'denial',\n",
       " 'about',\n",
       " 'faith',\n",
       " 'need',\n",
       " 'well',\n",
       " 'just',\n",
       " 'pretend',\n",
       " 'that',\n",
       " 'will',\n",
       " 'happily',\n",
       " 'ever',\n",
       " 'after',\n",
       " 'anyway',\n",
       " 'maybe',\n",
       " 'start',\n",
       " 'newsgroup',\n",
       " 'atheist',\n",
       " 'hard',\n",
       " 'bummin',\n",
       " 'much',\n",
       " 'forget',\n",
       " 'your',\n",
       " 'flintstone',\n",
       " 'chewables',\n",
       " 'bake',\n",
       " 'timmons']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "tokenized_doc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[1])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-56fa5dc91918>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m                                                        max_features)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[1;32m   1110\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 20, smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0738252efd73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msvd_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'randomized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m122\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msvd_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e4b0d6d34a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvd_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.shape(svd_model.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-45fb72177e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Topic %d:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \"\"\"\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         return [t for t, i in sorted(self.vocabulary_.items(),\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() \n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tokenized_doc' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8ef0de0cd6e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# LSA vs LDA : decrese the dimension of DTM vs word probability in topic + topic probability in document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtokenized_doc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_doc' is not defined"
     ]
    }
   ],
   "source": [
    "# LDA (Latent Dirichlet Allocation) : combination of topics based on probability distribution\n",
    "# trace the process of document (reverse engineering)\n",
    "# https://lettier.com/projects/lda-topic-modeling/\n",
    "\n",
    "# to find which topic exists from documents\n",
    "# LDA's input : count based DTM (From BoW) , TF-IDF\n",
    "\n",
    "# LSA vs LDA : decrese the dimension of DTM vs word probability in topic + topic probability in document\n",
    "tokenized_doc[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-11a73e9b6893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mNUM_TOPICS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mldamodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_TOPICS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 20 \n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-063ee2783176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "    if i==5:\n",
    "        break\n",
    "    print(i,'th document topic proportion',topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): \n",
    "            if j == 0:  \n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "source": [
    "topictable = make_topictable_per_doc(ldamodel, corpus)\n",
    "topictable = topictable.reset_index() # 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\n",
    "topictable.columns = ['문서 번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중', '각 토픽의 비중']\n",
    "topictable[:10]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 44,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ldamodel' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-ecc54f31a05a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopictable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_topictable_per_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtopictable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopictable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 문서 번호을 의미하는 열(column)로 사용하기 위해서 인덱스 열을 하나 더 만든다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopictable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'문서 번호'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'가장 비중이 높은 토픽'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'가장 높은 토픽의 비중'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'각 토픽의 비중'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtopictable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ldamodel' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/franciscadias/data/master/abcnews-date-text.csv\", filename=\"abcnews-date-text.csv\")\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1082168\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   publish_date                                      headline_text\n0      20030219  aba decides against community broadcasting lic...\n1      20030219     act fire witnesses must be aware of defamation\n2      20030219     a g calls for infrastructure protection summit\n3      20030219           air nz staff in aust strike for pay rise\n4      20030219      air nz strike to affect australian travellers\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>aba decides against community broadcasting lic...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>act fire witnesses must be aware of defamation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a g calls for infrastructure protection summit</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>air nz staff in aust strike for pay rise</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>air nz strike to affect australian travellers</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "text = data[['headline_text']]\n",
    "text.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text['headline_text'] = text.apply(lambda row : nltk.word_tokenize(row['headline_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  headline_text\n0              \n1              \n2              \n3              \n4              \n"
     ]
    }
   ],
   "source": [
    "print(text.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [word for word in x if word not in (stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                       headline_text\n0   [aba, decides, community, broadcasting, licence]\n1    [act, fire, witnesses, must, aware, defamation]\n2     [g, calls, infrastructure, protection, summit]\n3          [air, nz, staff, aust, strike, pay, rise]\n4  [air, nz, strike, affect, australian, travellers]\n"
     ]
    }
   ],
   "source": [
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                       headline_text\n",
      "0  [d, e, c, i, d, e, s,  , c, o, m, m, u, n, i, ...\n",
      "1  [f, i, r, e,  , w, i, t, n, e, s, s, e, s,  , ...\n",
      "2  [c, a, l, l, s,  , i, n, f, r, a, s, t, r, u, ...\n",
      "3  [s, t, a, f, f,  , a, u, s, t,  , s, t, r, i, ...\n",
      "4  [s, t, r, i, k, e,  , a, f, f, e, c, t,  , a, ...\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "text['headline_text'] = text['headline_text'].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "print(text.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    []\n1    []\n2    []\n3    []\n4    []\nName: headline_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenized_doc = text['headline_text'].apply(lambda x: [word for word in x if len(word) > 3])\n",
    "print(tokenized_doc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  import sys\n"
     ]
    }
   ],
   "source": [
    "# detokenization (token to text data again)\n",
    "detokenized_doc = []\n",
    "for i in range(len(text)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "text['headline_text'] = detokenized_doc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    \n",
       "1    \n",
       "2    \n",
       "3    \n",
       "4    \n",
       "Name: headline_text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "text['headline_text'][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000) # top 1000 words\n",
    "X = vectorizer.fit_transform(text['headline_text'])\n",
    "X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=10,learning_method='online',random_state=777,max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_top=lda_model.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.components_)\n",
    "print(lda_model.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names() # word sets. save 1000 vocabulary\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1])\n",
    "get_topics(lda_model.components_,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in components:\n",
    "    print(topic.argsort()[:-n - 1:-1])"
   ]
  }
 ]
}